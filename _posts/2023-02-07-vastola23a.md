---
title: Is the information geometry of probabilistic population codes learnable?
abstract: One reason learning the geometry of latent neural manifolds from neural
  activity data is difficult is that the ground truth is generally not known, which
  can make manifold learning methods hard to evaluate. Probabilistic population codes
  (PPCs), a class of biologically plausible and self-consistent models of neural populations
  that encode parametric probability distributions, may offer a theoretical setting
  where it is possible to rigorously study manifold learning. It is natural to define
  the neural manifold of a PPC as the statistical manifold of the encoded distribution,
  and we derive a mathematical result that the information geometry of the statistical
  manifold is directly related to measurable covariance matrices. This suggests a
  simple but rigorously justified decoding strategy based on principal component analysis,
  which we illustrate using an analytically tractable PPC.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vastola23a
month: 0
tex_title: Is the information geometry of probabilistic population codes learnable?
firstpage: 258
lastpage: 277
page: 258-277
order: 258
cycles: false
bibtex_author: Vastola, John J. and Cohen, Zach and Drugowitsch, Jan
author:
- given: John J.
  family: Vastola
- given: Zach
  family: Cohen
- given: Jan
  family: Drugowitsch
date: 2023-02-07
address:
container-title: Proceedings of the 1st NeurIPS Workshop on Symmetry and Geometry
  in Neural Representations
volume: '197'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 2
  - 7
pdf: https://proceedings.mlr.press/v197/vastola23a/vastola23a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
